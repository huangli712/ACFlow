# Used for objectives and solvers where the gradient is available/exists
mutable struct OnceDifferentiable
    â„±! # objective
    ğ’¥! # (partial) derivative of objective
    ğ¹  # cache for f output
    ğ½  # cache for j output
end

function OnceDifferentiable(ğ‘“, p0::AbstractArray, ğ¹::AbstractArray)
    function â„±!(ğ¹, x)
        copyto!(ğ¹, ğ‘“(x))
    end

    function ğ’¥!(ğ½, x)
        rel_step = cbrt(eps(real(eltype(x))))
        abs_step = rel_step
        @inbounds for i âˆˆ 1:length(x)
            xâ‚› = x[i]
            Ïµ = max(rel_step * abs(xâ‚›), abs_step)
            x[i] = xâ‚› + Ïµ
            fâ‚‚ = vec(ğ‘“(x))
            x[i] = xâ‚› - Ïµ
            fâ‚ = vec(ğ‘“(x))
            ğ½[:,i] = (fâ‚‚ - fâ‚) ./ (2 * Ïµ)
            x[i] = xâ‚›
        end
    end

    ğ½ = eltype(p0)(NaN) .* vec(ğ¹) .* vec(p0)'
    OnceDifferentiable(â„±!, ğ’¥!, ğ¹, ğ½)
end

value(obj::OnceDifferentiable) = obj.ğ¹
value(obj::OnceDifferentiable, ğ¹, x) = obj.â„±!(ğ¹, x)
function value!(obj::OnceDifferentiable, x)
    obj.â„±!(obj.ğ¹, x)
    obj.ğ¹
end

jacobian(obj::OnceDifferentiable) = obj.ğ½
jacobian(obj::OnceDifferentiable, ğ½, x) = obj.ğ’¥!(ğ½, x)
function jacobian!(obj::OnceDifferentiable, x)
    obj.ğ’¥!(obj.ğ½, x)
    obj.ğ½
end

struct OptimizationResults{T,N}
    xâ‚€::Array{T,N}
    minimizer::Array{T,N}
    minimum::T
    iterations::Int
    iteration_converged::Bool
    x_converged::Bool
    g_converged::Bool
end

"""
    levenberg_marquardt(df::OnceDifferentiable, xâ‚€::AbstractVector{T})

Returns the argmin over x of `sum(f(x).^2)` using the Levenberg-Marquardt
algorithm, and an estimate of the Jacobian of `f` at x. The function `f`
is encoded in `df`. `xâ‚€` is an initial guess for the solution.

See also: [`OnceDifferentiable`](@ref).
"""
function levenberg_marquardt(df::OnceDifferentiable, xâ‚€::AbstractVector{T}) where T
    # Some constants
    Î›â‚˜ = 1e16 # minimum trust region radius
    Î»â‚˜ = 1e-16 # maximum trust region radius
    min_diagonal = 1e-6 # lower bound on values of diagonal matrix
    x_tol = 1e-8 # search tolerance in x
    g_tol = 1e-12 # search tolerance in gradient
    maxIter = 1000 # maximum number of iterations
    Î» = T(10) # (inverse of) initial trust region radius
    Î»áµ¢ = 10.0 # Î» is multiplied by this factor after step below min quality
    Î»áµ£ = 0.1 # Î» is multiplied by this factor after good quality steps
    min_step_quality = 1e-3 # for steps below this quality, the trust region is shrinked
    good_step_quality = 0.75 # for steps above this quality, the trust region is expanded

    # First evaluation
    value!(df, xâ‚€)
    jacobian!(df, xâ‚€)
    ğ¹ = value(df)
    ğ½ = jacobian(df)

    converged = false
    x_converged = false
    g_converged = false
    iter = 0
    x = copy(xâ‚€)

    trial_f = similar(ğ¹)
    C_resid = sum(abs2, ğ¹)

    # Create buffers
    ğ½áµ€ğ½ = diagm(x)
    ğ½Î´x = similar(ğ¹)

    while (~converged && iter < maxIter)
        # Update jacobian ğ½
        jacobian!(df, x)

        # Solve the equation: [ğ½áµ€ğ½ + Î» diag(ğ½áµ€ğ½)] Î´ = ğ½áµ€ğ¹
        mul!(ğ½áµ€ğ½, ğ½', ğ½)
        ğ·áµ€ğ· = diag(ğ½áµ€ğ½)
        replace!(x -> x â‰¤ min_diagonal ? min_diagonal : x, ğ·áµ€ğ·)
        @simd for i in eachindex(ğ·áµ€ğ·)
            @inbounds ğ½áµ€ğ½[i,i] += Î» * ğ·áµ€ğ·[i]
        end
        Î´x = - ğ½áµ€ğ½ \ (ğ½' * ğ¹)

        # If the linear assumption is valid, our new residual should be:
        mul!(ğ½Î´x, ğ½, Î´x)
        ğ½Î´x .= ğ½Î´x .+ ğ¹
        P_resid = sum(abs2, ğ½Î´x)

        # Try to calculate new x, and then ğ¹, and then the residual.
        xnew = x + Î´x
        value(df, trial_f, xnew)
        T_resid = sum(abs2, trial_f)

        # Step quality = residual change / predicted residual change
        rho = (T_resid - C_resid) / (P_resid - C_resid)
        if rho > min_step_quality
            # Update x, ğ¹, and residual
            x .= xnew
            value!(df, x)
            C_resid = T_resid

            # Increase trust region radius
            if rho > good_step_quality
                Î» = max(Î»áµ£ * Î», Î»â‚˜)
            end
        else
            # Decrease trust region radius
            Î» = min(Î»áµ¢ * Î», Î›â‚˜)
        end

        # Increase the iteration
        iter += 1

        # Check convergence criteria:
        # 1. Small gradient: norm(ğ½áµ€ * ğ¹, Inf) < g_tol
        if norm(ğ½' * ğ¹, Inf) < g_tol
            g_converged = true
        end
        # 2. Small step size: norm(Î´x) < x_tol
        if norm(Î´x) < x_tol * (x_tol + norm(x))
            x_converged = true
        end
        converged = g_converged | x_converged
    end

    # Return the results
    OptimizationResults(
        xâ‚€,          # xâ‚€
        x,           # minimizer
        C_resid,     # minimum (residual)
        iter,        # iterations
        !converged,  # iteration_converged
        x_converged, # x_converged
        g_converged, # g_converged
    )
end

struct LsqFitResult
    param
    resid
    jacobian
    converged
end

"""
    curve_fit(model, x, y, p0)

Fit data to a non-linear `model`. `p0` is an initial model parameter guess.
The return object is a composite type (`LsqFitResult`).
"""
function curve_fit(model, x::AbstractArray, y::AbstractArray, p0::AbstractArray)
    f = (p) -> model(x, p) - y
    r = f(p0)
    R = OnceDifferentiable(f, p0, r)
    results = levenberg_marquardt(R, p0)
    p = results.minimizer
    conv = results.x_converged || results.g_converged
    return LsqFitResult(p, value!(R, p), jacobian!(R, p), conv)
end
