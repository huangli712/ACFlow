#
# Project : Gardenia
# Source  : nac.jl
# Author  : Li Huang (huangli@caep.cn)
# Status  : Unstable
#
# Last modified: 2023/11/03
#

#=
### *Customized Structs* : *NevanAC Solver*
=#

"""
    NevanACContext

Mutable struct. It is used within the NevanAC solver only.

### Members

* Gáµ¥   -> Input data for correlator.
* grid -> Grid for input data.
* mesh -> Mesh for output spectrum.
* Î¦    -> `Î¦` vector in Schur algorithm.
* ğ’œ    -> Coefficients matrix `abcd` in Schur algorithm.
* â„‹    -> Hardy matrix for Hardy basis optimization.
* ğ‘ğ‘   -> Coefficients matrix for expanding `Î¸` with Hardy basis.
* hmin -> Minimal value of the order of Hardy basis functions.
* hopt -> Optimal value of the order of Hardy basis functions.
"""
mutable struct NevanACContext
    Gáµ¥   :: Vector{APC}
    grid :: AbstractGrid
    mesh :: AbstractMesh
    Î¦    :: Vector{APC}
    ğ’œ    :: Array{APC,3}
    â„‹    :: Array{APC,2}
    ğ‘ğ‘   :: Vector{C64}
    hmin :: I64
    hopt :: I64
end

#=
### *Global Drivers*
=#

"""
    solve(S::NevanACSolver, rd::RawData)

Solve the analytic continuation problem by the Nevanlinna analytical
continuation method.
"""
function solve(S::NevanACSolver, rd::RawData)
    println("[ NevanAC ]")
    nac = init(S, rd)
    run(nac)
    Aout, Gout = last(nac)
    return nac.mesh.mesh, Aout, Gout
end

"""
    init(S::NevanACSolver, rd::RawData)

Initialize the NevanAC solver and return a NevanACContext struct.
"""
function init(S::NevanACSolver, rd::RawData)
    # Setup numerical precision. Note that the NAC method is extremely
    # sensitive to the float point precision.
    setprecision(128)

    # Convert the input data to APC, i.e., Complex{BigFloat}.
    Ï‰â‚™ = APC.(rd._grid * im)
    Gâ‚™ = APC.(rd.value)

    # Evaluate the optimal value for the size of input data.
    # Here we just apply the Pick criterion.
    ngrid = calc_noptim(Ï‰â‚™, Gâ‚™)

    # Prepera input data
    Gáµ¥ = calc_mobius(-Gâ‚™[1:ngrid])
    reverse!(Gáµ¥)
    println("Postprocess input data: ", length(Gáµ¥), " points")

    # Prepare grid for input data
    grid = make_grid(rd, T = APF)
    resize!(grid, ngrid)
    reverse!(grid)
    println("Build grid for input data: ", length(grid), " points")

    # Prepare mesh for output spectrum
    mesh = make_mesh(T = APF)
    println("Build mesh for spectrum: ", length(mesh), " points")

    # Precompute key quantities to accelerate the computation
    Î¦, ğ’œ, â„‹, ğ‘ğ‘ = precompute(grid, mesh, Gáµ¥)
    println("Precompute key matrices")

    # Actually, the NevanACContext struct already contains enough
    # information to build the Nevanlinna interpolant and get the
    # spectrum, but Hardy basis optimization is needed to smooth
    # the results further.
    return NevanACContext(Gáµ¥, grid, mesh, Î¦, ğ’œ, â„‹, ğ‘ğ‘, 1, 1)
end

"""
    run(nac::NevanACContext)

Perform Hardy basis optimization to smooth the spectrum. the members `â„‹`,
`ğ‘ğ‘`, `hmin`, and `hopt` of the NevanACContext object (`nac`) should be
updated in this function.
"""
function run(nac::NevanACContext)
    hardy = get_n("hardy")
    #
    if hardy
        # Determine the minimal Hardy order (`hmin`), update `â„‹` and `ğ‘ğ‘`.
        calc_hmin!(nac)

        # Determine the optimal Hardy order (`hopt`), update `â„‹` and `ğ‘ğ‘`.
        calc_hopt!(nac)
    end
end

"""
    last(nac::NevanACContext)

Postprocess the results generated during the Nevanlinna analytical
continuation simulations.
"""
function last(nac::NevanACContext)
    # By default, we should write the analytic continuation results
    # into the external files.
    _fwrite = get_b("fwrite")
    fwrite = isa(_fwrite, Missing) || _fwrite ? true : false

    # Calculate full response function on real axis and write them
    # Note that _G is actually ğ‘G, so there is a `-` symbol for the
    # return value.
    _G = C64.(calc_green(nac.ğ’œ, nac.â„‹, nac.ğ‘ğ‘))
    fwrite && write_complete(nac.mesh, -_G)

    # Calculate and write the spectral function
    Aout = F64.(imag.(_G) ./ Ï€)
    fwrite && write_spectrum(nac.mesh, Aout)

    # Regenerate the input data and write them
    kernel = make_kernel(nac.mesh, nac.grid)
    G = reprod(nac.mesh, kernel, Aout)
    fwrite && write_backward(nac.grid, G)

    return Aout, -_G
end

#=
### *Service Functions*
=#

#=
*Remarks* :

**Mobius transformation**

```math
\begin{equation}
z \mapsto \frac{z - i}{z + i}.
\end{equation}
```

It maps `z` from ``\overline{\mathcal{C}^{+}}`` to ``\overline{\mathcal{D}}``.
See `calc_mobius()`.

---

**Inverse Mobius transformation**

```math
\begin{equation}
z \mapsto i \frac{1 + z}{1 - z}.
\end{equation}
```

It maps `z` from ``\overline{\mathcal{D}}`` to ``\overline{\mathcal{C}^{+}}``.
See `calc_inv_mobius()`.

---

**Pick matrix**

```math
\begin{equation}
\mathcal{P} = 
\left[
    \frac{1-\lambda_i \lambda^*_j}{1-h(Y_i)h(Y_j)^*}
\right]_{i,j}.
\end{equation}
```

Here, ``Y_i`` is the *i*th Matsubara frequency, ``C_i`` is the value of
``\mathcal{NG}`` at ``Y_i``, and ``\lambda_i`` is the value of ``\theta``
at ``Y_i``:

```math
\begin{equation}
\lambda_i = \theta(Y_i) = \frac{C_i - i}{C_i + i},
\end{equation}
```

```math
\begin{equation}
h(Y_i) = \frac{Y_i - i}{Y_i + i}.
\end{equation}
```

See `calc_pick()`.

---

**Î¦ vector**

At first, we have

```math
\begin{equation}
\phi_{\alpha} = \theta_{\alpha}(Y_{\alpha}).
\end{equation}
```

So

```math
\begin{equation}
\phi_1 = \theta_1 (Y_1),
\end{equation}
```

```math
\begin{equation}
\phi_{\beta} =
\frac{-d_{\beta}\theta(Y_{\beta}) + b_{\beta}}{c_{\beta}\theta(Y_{\beta}) - \alpha_{\beta}}
\end{equation}
```

where

```math
\begin{equation}
\begin{pmatrix}
a_{\beta} & b_{\beta} \\
c_{\beta} & d_{\beta}
\end{pmatrix}
= \prod^{\beta-1}_{\alpha=1}
\begin{pmatrix}
\frac{Y_{\beta}-Y_{\alpha}}{Y_{\beta}-Y^*_{\alpha}} & \phi_{\alpha} \\
\phi^*_{\alpha}\frac{Y_{\beta} - Y_{\alpha}}{Y_{\beta} - Y^*_{\alpha}} & 1
\end{pmatrix},
\end{equation}
```

See `calc_phis()`.

---

**Contractive function Î¸(z)**

```math
\begin{equation}
\theta(z)[z; Î¸_{M+1}(z)] =
\frac{a(z)\theta_{M+1}(z) + b(z)}{c(z)\theta_{M+1}(z) + d(z)}
\end{equation}
```

where

```math
\begin{equation}
\begin{pmatrix}
a(z) & b(z) \\
c(z) & d(z) 
\end{pmatrix}
= \prod^{M}_{j=1}
\begin{pmatrix}
\frac{z - Y_j}{z - Y^*_j} & \phi_j \\
\phi^*_j \frac{z - Y_j}{z - Y^*_j} & 1
\end{pmatrix}
\end{equation}
```

See `calc_abcd()` and `calc_theta()`.

---

**Hardy basis**

```math
\begin{equation}
f^k(z) = \frac{1}{\sqrt{\pi}(z + i)}
    \left( \frac{z - i}{z + i} \right)^k.
\end{equation}
```

See `calc_hbasis()` and `calc_hmatrix()`.

---

**Expanding ``\theta_{M+1}`` using Hardy basis**

```math
\theta_{M+1} = \sum^{H}_{k=0} \left[a_k f^k(z) + b_k f^k(z)^*\right]
```

See `calc_theta()`.

---

**Smooth norm**

```math
\begin{equation}
F[A_{\theta_{M+1}}(\omega)] =
    \left|
        1 - \int A_{\theta_{M+1}}(\omega) d\omega
    \right|^2 +
    \alpha \int \left[A^{''}_{\theta_{M+1}}(\omega)\right]^2 d\omega,
\end{equation}
```
where the first term enforces proper normalization while the second
term promotes smoothness by minimizing second derivatives. Here Î±
is a regulation parameter.

See `smooth_norm()`.
=#

"""
    precompute(grid::AbstractGrid,
               mesh::AbstractMesh,
               Gáµ¥::Vector{APC})

Precompute some key quantities, such as `Î¦`, `ğ’œ`, `â„‹`, and `ğ‘ğ‘`. Note
that `Î¦` and `ğ’œ` won't be changed any more. But `ğ’œ` and `ğ‘ğ‘` should be
updated by the Hardy basis optimization to get a smooth spectrum. Here
`Gáµ¥` is input data, `grid` is the grid for input data, and `mesh` is
the mesh for output spectrum.
"""
function precompute(grid::AbstractGrid,
                    mesh::AbstractMesh,
                    Gáµ¥::Vector{APC})
    # Evaluate Ï• and `abcd` matrices
    Î¦ = calc_phis(grid, Gáµ¥)
    ğ’œ = calc_abcd(grid, mesh, Î¦)

    # Allocate memory for evaluating Î¸
    # The initial Hardy order is just 1.
    â„‹ = calc_hmatrix(mesh, 1)
    ğ‘ğ‘ = zeros(C64, 2)

    return Î¦, ğ’œ, â„‹, ğ‘ğ‘
end

"""
    calc_mobius(z::Vector{APC})

A direct Mobius transformation.
"""
function calc_mobius(z::Vector{APC})
    return @. (z - im) / (z + im)
end

"""
    calc_inv_mobius(z::Vector{APC})

An inverse Mobius transformation.
"""
function calc_inv_mobius(z::Vector{APC})
    return @. im * (one(APC) + z) / (one(APC) - z)
end

"""
    calc_pick(k::I64, â„::Vector{APC}, Î»::Vector{APC})

Try to calculate the Pick matrix, anc check whether it is a positive
semidefinite matrix. See Eq. (5) in Fei's NAC paper.
"""
function calc_pick(k::I64, â„::Vector{APC}, Î»::Vector{APC})
    pick = zeros(APC, k, k)

    # Calculate the Pick matrix
    for j = 1:k
        for i = 1:k
            num = one(APC) - Î»[i] * conj(Î»[j])
            den = one(APC) - â„[i] * conj(â„[j])
            pick[i,j] = num / den
        end
        pick[j,j] += APC(1e-250)
    end

    # Cholesky decomposition
    return issuccess(cholesky(pick, check = false))
end

"""
    calc_phis(grid::AbstractGrid, Gáµ¥::Vector{APC})

Try to calculate the Î¦ vector, which is used to calculate the ğ’œ matrix.
Note that Î¦ should not be changed anymore once it has been established.
"""
function calc_phis(grid::AbstractGrid, Gáµ¥::Vector{APC})
    ngrid = length(grid)

    # Allocate memory
    Î¦ = zeros(APC, ngrid) 
    ğ’œ = zeros(APC, 2, 2, ngrid)
    âˆ = zeros(APC, 2, 2)
    ğ‘” = grid.Ï‰ * im

    # Initialize the `abcd` matrix
    for i = 1:ngrid
        ğ’œ[:,:,i] .= Matrix{APC}(I, 2, 2)
    end

    # Evaluate Î¦ using recursive algorithm
    Î¦[1] = Gáµ¥[1]
    for j = 1:ngrid-1
        for k = j+1:ngrid
            âˆ[1,1] = ( ğ‘”[k] - ğ‘”[j] ) / ( ğ‘”[k] - conj(ğ‘”[j]) )
            âˆ[1,2] = Î¦[j]
            âˆ[2,1] = conj(Î¦[j]) * âˆ[1,1]
            âˆ[2,2] = one(APC)
            view(ğ’œ,:,:,k) .= view(ğ’œ,:,:,k) * âˆ
        end
        num = ğ’œ[1,2,j+1] - ğ’œ[2,2,j+1] * Gáµ¥[j+1]
        den = ğ’œ[2,1,j+1] * Gáµ¥[j+1] - ğ’œ[1,1,j+1]
        Î¦[j+1] = num / den
    end

    return Î¦
end

"""
    calc_abcd(grid::AbstractGrid, mesh::AbstractMesh, Î¦::Vector{APC})

Try to calculate the coefficients matrix abcd (here it is called ğ’œ),
which is then used to calculate Î¸. See Eq. (8) in Fei's NAC paper.
"""
function calc_abcd(grid::AbstractGrid, mesh::AbstractMesh, Î¦::Vector{APC})
    eta::APF = get_n("eta")

    ngrid = length(grid)
    nmesh = length(mesh)

    ğ‘” = grid.Ï‰ * im
    ğ‘š = mesh.mesh .+ im * eta

    ğ’œ = zeros(APC, 2, 2, nmesh)
    âˆ = zeros(APC, 2, 2)

    for i = 1:nmesh
        result = Matrix{APC}(I, 2, 2)
        ğ‘§ = ğ‘š[i]
        for j = 1:ngrid
            âˆ[1,1] = ( ğ‘§ - ğ‘”[j] ) / ( ğ‘§ - conj(ğ‘”[j]) )
            âˆ[1,2] = Î¦[j]
            âˆ[2,1] = conj(Î¦[j]) * âˆ[1,1]
            âˆ[2,2] = one(APC)
            result *= âˆ
        end

        ğ’œ[:,:,i] .= result
    end

    return ğ’œ
end

"""
    calc_hbasis(z::APC, k::I64)

Try to calculate the Hardy basis ``f^k(z)``.
"""
function calc_hbasis(z::APC, k::I64)
    w = ( z - im ) / ( z + im )
    return 1.0 / ( sqrt(Ï€) * (z + im) ) * w^k
end

"""
    calc_hmatrix(mesh::AbstractMesh, H::I64)

Try to calculate ``[f^k(z), f^k(z)^*]`` for 0 â‰¤ ğ‘˜ â‰¤ ğ»-1, which is
called the hardy matrix (â„‹) and is used to evaluate ``\theta_{M+1}``.
"""
function calc_hmatrix(mesh::AbstractMesh, H::I64)
    # Build real axis
    eta::APF = get_n("eta")
    ğ‘š = mesh.mesh .+ eta * im

    # Allocate memory for the Hardy matrix
    nmesh = length(mesh)
    â„‹ = zeros(APC, nmesh, 2*H)

    # Build the Hardy matrix
    for k = 1:H
        â„‹[:,2*k-1] .= calc_hbasis.(ğ‘š,k-1)
        â„‹[:,2*k]   .= conj(â„‹[:,2*k-1])
    end

    return â„‹
end

"""
    calc_theta(ğ’œ::Array{APC,3}, â„‹::Array{APC,2}, ğ‘ğ‘::Vector{C64})

Try to calculate the contractive function Î¸(z). ğ’œ is the coefficients
matrix abcd, â„‹ is the Hardy matrix, and ğ‘ğ‘ are complex coefficients
for expanding Î¸â‚˜â‚Šâ‚. See Eq. (7) in Fei's NAC paper.
"""
function calc_theta(ğ’œ::Array{APC,3}, â„‹::Array{APC,2}, ğ‘ğ‘::Vector{C64})
    # Well, we should calculate Î¸â‚˜â‚Šâ‚ at first.
    Î¸â‚˜â‚Šâ‚ = â„‹ * ğ‘ğ‘

    # Then we evaluate Î¸ according Eq. (7)
    num = ğ’œ[1,1,:] .* Î¸â‚˜â‚Šâ‚ .+ ğ’œ[1,2,:]
    den = ğ’œ[2,1,:] .* Î¸â‚˜â‚Šâ‚ .+ ğ’œ[2,2,:]
    Î¸ = num ./ den

    return Î¸
end

"""
    calc_green(ğ’œ::Array{APC,3}, â„‹::Array{APC,2}, ğ‘ğ‘::Vector{C64})

Firstly we try to calculate Î¸. Then Î¸ is back transformed to a Nevanlinna
interpolant via the inverse Mobius transform. Here, `ğ’œ` (`abcd` matrix),
`â„‹` (Hardy matrix), and `ğ‘ğ‘` are used to evaluate Î¸.
"""
function calc_green(ğ’œ::Array{APC,3}, â„‹::Array{APC,2}, ğ‘ğ‘::Vector{C64})
    Î¸ = calc_theta(ğ’œ, â„‹, ğ‘ğ‘)
    gout = calc_inv_mobius(Î¸)

    return gout
end

"""
    calc_noptim(Ï‰â‚™::Vector{APC}, Gâ‚™::Vector{APC})

Evaluate the optimal value for the size of input data (how may frequency
points are actually used in the analytic continuation simulations) via
the Pick criterion.
"""
function calc_noptim(Ï‰â‚™::Vector{APC}, Gâ‚™::Vector{APC})
    # Get size of input data
    ngrid = length(Ï‰â‚™)

    # Check whether the Pick criterion is applied 
    pick = get_n("pick")
    if !pick
        return ngrid
    end

    # Apply invertible Mobius transformation. We actually work at
    # the \bar{ğ’Ÿ} space.
    ğ“ = calc_mobius(Ï‰â‚™)
    ğ’¢ = calc_mobius(-Gâ‚™)

    # Find the optimal value of k until the Pick criterion is violated
    k = 0
    success = true
    while success && k â‰¤ ngrid
        k += 1
        success = calc_pick(k, ğ“, ğ’¢)
    end

    # Return the optimal value for the size of input data
    if !success
        println("The size of input data is optimized to $(k-1)")
        return k - 1
    else
        println("The size of input data is optimized to $(ngrid)")
        return ngrid
    end
end

"""
    calc_hmin!(nac::NevanACContext)

Try to perform Hardy basis optimization. Such that the Hardy matrix â„‹
and the corresponding coefficients ğ‘ğ‘ are updated. They are used to
calculate Î¸, which is then back transformed to generate smooth G (i.e.,
the spectrum) at real axis.

This function will determine the minimal value of H (hmin). Of course,
â„‹ and ğ‘ğ‘ in NevanACContext object are also changed.
"""
function calc_hmin!(nac::NevanACContext)
    hmax = get_n("hmax")

    h = 1
    while h â‰¤ hmax
        println("H = $h")

        # Prepare initial â„‹ and ğ‘ğ‘
        â„‹ = calc_hmatrix(nac.mesh, h)
        ğ‘ğ‘ = zeros(C64, 2*h)

        # Hardy basis optimization
        causality, optim = hardy_optimize!(nac, â„‹, ğ‘ğ‘, h)

        # Check whether the causality is preserved and the
        # optimization is successful.
        if causality && optim
            nac.hmin = h
            break
        else
            h = h + 1
        end
    end
end

"""
    calc_hopt!(nac::NevanACContext)

Try to perform Hardy basis optimization. Such that the Hardy matrix â„‹
and the corresponding coefficients ğ‘ğ‘ are updated. They are used to
calculate Î¸, which is then back transformed to generate smooth G (i.e.,
the spectrum) at real axis.

This function will determine the optimal value of H (hopt). Of course,
â„‹ and ğ‘ğ‘ in NevanACContext object are also changed.
"""
function calc_hopt!(nac::NevanACContext)
    hmax = get_n("hmax")
    hmax = 3 # DEBUG

    for h = nac.hmin + 1:hmax
        println("H = $h")

        # Prepare initial â„‹ and ğ‘ğ‘
        â„‹ = calc_hmatrix(nac.mesh, h)
        ğ‘ğ‘  = copy(nac.ğ‘ğ‘)
        push!(ğ‘ğ‘, zero(C64))
        push!(ğ‘ğ‘, zero(C64))
        @assert size(â„‹)[2] == length(ğ‘ğ‘)

        # Hardy basis optimization
        causality, optim = hardy_optimize!(nac, â„‹, ğ‘ğ‘, h)

        # Check whether the causality is preserved and the
        # optimization is successful.
        if !(causality && optim)
            break
        end
    end
end

"""
    hardy_optimize!(nac::NevanACContext,
                    â„‹::Array{APC,2},
                    ğ‘ğ‘::Vector{C64},
                    H::I64)

For given Hardy matrix â„‹, try to update the expanding coefficients ğ‘ğ‘
by minimizing the smooth norm.
"""
function hardy_optimize!(nac::NevanACContext,
                         â„‹::Array{APC,2},
                         ğ‘ğ‘::Vector{C64},
                         H::I64)
    function ğ‘“(x::Vector{C64})
        return smooth_norm(nac, â„‹, x)
    end

    function ğ½!(J::Vector{C64}, x::Vector{C64})
        J .= gradient(ğ‘“, x)
    end

    res = optimize(ğ‘“, ğ½!, ğ‘ğ‘, max_iter = 500)
    
    @show res.minimizer

    if  !(converged(res))
        println("Faild to optimize!")
    end
    
    causality = check_causality(â„‹, res.minimizer)

    if causality && (converged(res))
        nac.hopt = H
        nac.ğ‘ğ‘ = res.minimizer
        nac.â„‹ = â„‹
    end
    
    return causality, (converged(res))
end

"""
    smooth_norm(nac::NevanACContext, â„‹::Array{APC,2}, ğ‘ğ‘::Vector{C64})

Establish the smooth norm, which is used to improve the smoothness of
the output spectrum.
"""
function smooth_norm(nac::NevanACContext, â„‹::Array{APC,2}, ğ‘ğ‘::Vector{C64})
    # Get regulation parameter
    Î± = get_n("alpha")

    # Generate output spectrum
    _G = calc_green(nac.ğ’œ, â„‹, ğ‘ğ‘)
    A = F64.(imag.(_G) ./ Ï€)

    # Normalization term
    ğ‘“â‚ = trapz(nac.mesh, A)

    # Smoothness term
    sd = deriv2(nac.mesh.mesh, A)
    x_sd = nac.mesh.mesh[2:end-1]
    ğ‘“â‚‚ = trapz(x_sd, abs.(sd) .^ 2)

    # Assemble the final smooth norm
    ğ¹ = abs(1.0 - ğ‘“â‚)^2 + Î± * ğ‘“â‚‚

    return F64(ğ¹)
end

"""
"""
function check_pick(wn::Vector{APC}, gw::Vector{APC}, Nopt::I64)
    freq = calc_mobius(wn[1:Nopt])
    val = calc_mobius(-gw[1:Nopt])

    success = calc_pick(Nopt, val, freq)
    if success
        println("Pick matrix is positive semi-definite.")
    else
        println("Pick matrix is non positive semi-definite matrix in Schur method.")
    end
    
    freq = reverse(wn[1:Nopt])
    val  = reverse(val)
end

"""
"""
function check_causality(â„‹::Array{APC,2}, ğ‘ğ‘::Vector{C64})
    param = â„‹ * ğ‘ğ‘
    @show typeof(param), size(â„‹), size(ğ‘ğ‘), size(param)

    max_theta = findmax(abs.(param))[1]
    if max_theta <= 1.0
        println("max_theta = ",max_theta)
        println("Hardy optimization was success.")
        causality = true
    else
        println("max_theta = ",max_theta)
        println("Hardy optimization was failure.")
        causality = false
    end

    return causality
end

mutable struct BFGSState{Tx, Tm, T, G}
    x::Tx
    x_previous::Tx
    g_previous::G
    f_x_previous::T
    dx::Tx
    dg::Tx
    u::Tx
    invH::Tm
    s::Tx
    x_ls::Tx
    alpha::T
end

mutable struct BFGSOptimizationResults{Tx, Tc, Tf}
    initial_x::Tx
    minimizer::Tx
    minimum::Tf
    iterations::Int
    x_abschange::Tc
    x_relchange::Tc
    f_abschange::Tc
    f_relchange::Tc
    g_converged::Bool
    g_residual::Tc
end

include("hagerzhang.jl")

# Used for objectives and solvers where the gradient is available/exists
mutable struct BFGSDifferentiable{TF, TDF}
    â„±! # objective, f
    ğ’Ÿ! # (partial) derivative of objective, df
    ğ¹ :: TF # cache for f output, F
    ğ· :: TDF # cache for df output, DF
end

x_of_nans(x, Tf=eltype(x)) = fill!(Tf.(x), Tf(NaN))
alloc_DF(x, F::T) where T<:Number = x_of_nans(x, promote_type(eltype(x), T))

function BFGSDifferentiable(f, df, x::AbstractArray)
    F::Real = real(zero(eltype(x)))
    DF::AbstractArray = alloc_DF(x, F)
    BFGSDifferentiable(f, df, copy(F), copy(DF))
end

value(obj::BFGSDifferentiable) = obj.ğ¹
gradient(obj::BFGSDifferentiable) = obj.ğ·
function value_gradient!(obj::BFGSDifferentiable, x)
    obj.ğ’Ÿ!(gradient(obj), x)
    obj.ğ¹ = obj.â„±!(x)
end

function init_state(d::BFGSDifferentiable, initial_x::AbstractArray{T}) where T
    value_gradient!(d, initial_x)

    x_ = reshape(initial_x, :)
    invH0 = x_ .* x_' .* false
    idxs = diagind(invH0)
    scale = eltype(initial_x)(1)
    @. @view(invH0[idxs]) = scale * true

    # Maintain a cache for line search results
    # Trace the history of states visited
    BFGSState(initial_x, # Maintain current state in state.x
              copy(initial_x), # Maintain previous state in state.x_previous
              copy(gradient(d)), # Store previous gradient in state.g_previous
              real(T)(NaN), # Store previous f in state.f_x_previous
              similar(initial_x), # Store changes in position in state.dx
              similar(initial_x), # Store changes in gradient in state.dg
              similar(initial_x), # Buffer stored in state.u
              invH0, # Store current invH in state.invH
              similar(initial_x), # Store current search direction in state.s
              similar(initial_x), # Buffer of x for line search in state.x_ls
              real(one(T))
    )
end

function update_state!(d::BFGSDifferentiable, state::BFGSState)
    T = eltype(state.s)
    # Set the search direction
    # Search direction is the negative gradient divided by the approximate Hessian
    mul!(vec(state.s), state.invH, vec(gradient(d)))
    rmul!(state.s, T(-1))

    # Maintain a record of the previous gradient
    copyto!(state.g_previous, gradient(d))

    # Determine the distance of movement along the search line
    # This call resets invH to initial_invH is the former in not positive
    # semi-definite
    lssuccess = perform_linesearch!(state, d)

    # Update current position
    state.dx .= state.alpha.*state.s
    state.x .= state.x .+ state.dx

    lssuccess == false # break on linesearch error
end

function trace!(d::BFGSDifferentiable, iteration, curr_time=time())
    dt = Dict()
    dt["time"] = curr_time
    g_norm = norm(gradient(d), Inf)

    if iteration % 1 == 0
        @printf("%6d   %14e   %14e\n", iteration, value(d), g_norm)
        if !isempty(dt)
            for (key, value) in dt
                @printf(" * %s: %s\n", key, value)
            end
        end
        flush(stdout)
    end
    false
end

# Update the function value and gradient
function update_g!(d::BFGSDifferentiable, state::BFGSState)
    value_gradient!(d, state.x)
end

function update_h!(d::BFGSDifferentiable, state::BFGSState)
    n = length(state.x)
    # Measure the change in the gradient
    state.dg .= gradient(d) .- state.g_previous

    # Update the inverse Hessian approximation using Sherman-Morrison
    dx_dg = real(dot(state.dx, state.dg))
    if dx_dg > 0
        mul!(vec(state.u), state.invH, vec(state.dg))

        c1 = (dx_dg + real(dot(state.dg, state.u))) / (dx_dg' * dx_dg)
        c2 = 1 / dx_dg

        # invH = invH + c1 * (s * s') - c2 * (u * s' + s * u')
        if(state.invH isa Array) # i.e. not a CuArray
            invH = state.invH; dx = state.dx; u = state.u;
            @inbounds for j in 1:n
                c1dxj = c1 * dx[j]'
                c2dxj = c2 * dx[j]'
                c2uj  = c2 *  u[j]'
                for i in 1:n
                    invH[i, j] = muladd(dx[i], c1dxj, muladd(-u[i], c2dxj, muladd(c2uj, -dx[i], invH[i, j])))
                end
            end
        else
            mul!(state.invH,vec(state.dx),vec(state.dx)', c1,1)
            mul!(state.invH,vec(state.u ),vec(state.dx)',-c2,1)
            mul!(state.invH,vec(state.dx),vec(state.u )',-c2,1)
        end
    end
end

function optimize(f, g, initial_x::AbstractArray; max_iter::I64 = 1000)
    d = BFGSDifferentiable(f, g, initial_x)
    state = init_state(d, initial_x)

    t0 = time() # Initial time stamp

    stopped = false

    g_converged = !isfinite(value(d)) || any(!isfinite, gradient(d)) #initial_convergence(d)
    # prepare iteration counter (used to make "initial state" trace entry)
    iteration = 0

    @printf "Iter     Function value   Gradient norm \n"

    _time = time()
    trace!(d, iteration, _time-t0)
    ls_success = true
    while !g_converged && !stopped && iteration < max_iter
        iteration += 1
        ls_success = !update_state!(d, state)
        if !ls_success
            break # it returns true if it's forced by something in update! to stop (eg dx_dg == 0.0 in BFGS, or linesearch errors)
        end
        update_g!(d, state)
        g_converged = (g_residual(d) â‰¤ 1e-8)
        update_h!(d, state) # only relevant if not converged

        # update trace
        trace!(d, iteration, time()-t0)

        _time = time()

        if !all(isfinite, gradient(d))
            @warn "Terminated early due to NaN in gradient."
            break
        end
    end # while

    # we can just check minimum, as we've earlier enforced same types/eltypes
    # in variables besides the option settings
    return BFGSOptimizationResults(initial_x,
                                        state.x,
                                        value(d),
                                        iteration,
                                        x_abschange(state),
                                        x_relchange(state),
                                        f_abschange(d, state),
                                        f_relchange(d, state),
                                        g_converged,
                                        g_residual(d)
    )
end

function perform_linesearch!(state::BFGSState, d::BFGSDifferentiable)
    # Calculate search direction dphi0
    dphi_0 = real(dot(gradient(d), state.s))
    # reset the direction if it becomes corrupted
    if dphi_0 >= zero(dphi_0)
        dphi_0 = real(dot(gradient(d), state.s)) # update after direction reset
    end
    phi_0  = value(d)

    # Guess an alpha
    guess = InitialStatic()
    linesearch = HagerZhang()
    guess(linesearch, state, phi_0, dphi_0, d)

    # Store current x and f(x) for next iteration
    state.f_x_previous = phi_0
    copyto!(state.x_previous, state.x)

    # Perform line search; catch LineSearchException to allow graceful exit
    try
        state.alpha, Ï•alpha = linesearch(d, state.x, state.s, state.alpha,
                               state.x_ls, phi_0, dphi_0)
        return true # lssuccess = true
    catch ex
        if isa(ex, LineSearchException)
            state.alpha = ex.alpha
            # We shouldn't warn here, we should just carry it to the output
            # @warn("Linesearch failed, using alpha = $(state.alpha) and
            # exiting optimization.\nThe linesearch exited with message:\n$(ex.message)")
            return false # lssuccess = false
        else
            rethrow(ex)
        end
    end
end

function maxdiff(x::AbstractArray, y::AbstractArray)
    return mapreduce((a, b) -> abs(a - b), max, x, y)
end

f_abschange(d::BFGSDifferentiable, state) = abs(value(d) - state.f_x_previous)
f_relchange(d::BFGSDifferentiable, state) = abs(value(d) - state.f_x_previous)/abs(value(d))
x_abschange(state::BFGSState) = maxdiff(state.x, state.x_previous)
x_relchange(state::BFGSState) = maxdiff(state.x, state.x_previous)/maximum(abs, state.x)
g_residual(d::BFGSDifferentiable) = g_residual(gradient(d))
g_residual(g) = maximum(abs, g)

function converged(r::BFGSOptimizationResults)
    conv_flags = r.g_converged
    x_isfinite = isfinite(r.x_abschange) || isnan(r.x_relchange)
    f_isfinite = if r.iterations > 0
            isfinite(r.f_abschange) || isnan(r.f_relchange)
        else
            true
        end
    g_isfinite = isfinite(r.g_residual)
    return conv_flags && all((x_isfinite, f_isfinite, g_isfinite))
end
